Operator-Aware Robust Calibration for Bus-Corridor Digital Twins via Bayesian Optimization and Iterative Ensemble Smoothing IEEE SMC - IEEE International Conference on Systems, Man, and Cybernetics Submitted: January 8, 2026 Contents Summary Strengths Weaknesses Detailed Comments Questions Overall Assessment Summary This paper proposes RCMDT, a two-level calibration framework for bus-corridor digital twins that separates stop-level behavioral parameter search (outer loop via Gaussian-process Bayesian optimization with a tail-aware loss) from corridor state reconciliation (inner loop via an iterative ensemble smoother). A key design choice is an “operator-aware” observation audit that filters door-to-door (D2D) samples into transport and non-transport regimes using fixed rules, and a strict separation of calibration and validation, with validation relying on distributional evidence (K–S distance) and worst-window stress tests. Experiments on two Hong Kong bus routes in SUMO suggest that the audit and the Bayesian–assimilation loop improve distributional alignment under next-day transfer while maintaining cross-corridor feasibility. Strengths Technical novelty and innovation A clear articulation of the “measurement semantics” problem in bus operations (holding, layover, schedule recovery) that contaminates D2D targets, and a principled attempt to address it via an auditable observation operator. The two-level architecture (BO for stop-behavior, IES for corridor states) that decouples microscopic parameters from macroscopic regime shifts is conceptually sound and aligns with best practices in data assimilation for CPS. A composite, tail-aware calibration objective that explicitly includes dispersion and a high-quantile penalty to promote reliability rather than pure mean fit. Experimental rigor and validation A no-reoptimization validation protocol and worst-window stress tests reflect a robustness mindset beyond single-number averages. Use of an external traffic-only product (IRN speeds) for plausibility checks strengthens the case for separating operational artifacts from traffic dynamics. Clarity of presentation The problem statement, high-level method, and the rationale for separating calibration/validation are well motivated and accessible. The flowchart and figure narratives effectively convey the proposed pipeline and audit logic. Significance of contributions Addressing reliability under distribution shift for human-centric transit systems is an important and timely topic for digital twins in CPS. The operator-audit framing has practical implications for transit agencies and vendors who face “ghost” tails and regime mixing in AVL-derived targets. Weaknesses Technical limitations or concerns The inner-loop IES is under-specified: state dimensionality/content, observation vector composition, ensemble size, assimilation windowing, iteration schedule, and covariance settings (R, process error) are not detailed, limiting reproducibility and technical assessment. Fixed audit thresholds (T* = 325 s, v* = 5 km/h) are justified qualitatively but lack sensitivity analysis or principled derivation; the fraction of flagged samples is large in at least one window (54.3%), raising concerns about over-filtering and generality. The decoupling claim is unclear: text alternately suggests audit is used to prevent optimizer misuse and that audit outputs do not change L1 calibration targets. If calibration still uses unaudited D2D, the method may remain vulnerable to semantics contamination. Experimental gaps or methodological issues Limited scope: only two routes in one city; no ablation isolating the incremental benefits of IES (on vs off), audit (on vs off), and BO/tail loss (vs conventional loss), nor comparisons to established calibration/assimilation baselines (e.g., AugEnKF, EnRML/IEnKS variants, robust outlier filtering). K–S “Pass/Fail” lacks statistical grounding (no n, m, p-values, or critical values); validation uses speed CDFs while calibration optimizes travel-time errors, creating a metric mismatch. Reported improvements (e.g., BO vs LHS 8.35%) are modest and not tied to downstream reliability metrics; distributional improvements after audit may primarily reflect data filtering rather than improved model fidelity. Clarity or presentation issues The composition of the “corridor observation vector” for L2 and the precise nature of “background states” are not specified; “traffic-only” vs “full-time” simulation usage remains ambiguous in evaluation. “Zero-shot simulation” is referenced but not defined in relation to calibrated vs uncalibrated runs. Missing related work or comparisons No benchmarking against modern iterative smoother/filter variants tailored for nonlinearity/multimodality (e.g., EnRML/IEnKS refinements, ILUES) or recent unified state–parameter filters (e.g., United Filter) that could be relevant alternatives to the proposed split-loop design. The paper would benefit from positioning relative to DT TEVV frameworks (continuous validation/tooling) and to recent metamodel-based calibration approaches in traffic that also emphasize distributional alignment and sample efficiency. Detailed Comments Technical soundness evaluation The two-level decomposition (behavioral parameters via BO and corridor states via IES) is a principled approach that avoids conflating regime shifts with stop-level behavior; however, the mathematical/implementation specifics of IES are too sparse for technical scrutiny (state definition, window size, ensemble size, iteration count, R/Pf modeling, feasibility projection). The composite calibration loss is reasonable for reliability, though its weighting (α, λ, β) is not discussed; without sensitivity checks it is difficult to assess robustness to weight choices. The operator-audit concept is sensible, but the choice of fixed thresholds requires empirical justification across multiple regime types and routes; otherwise, RCMDT may trade bias for variance by aggressively filtering. Experimental evaluation assessment The use of IRN speeds for plausibility checks is a solid diagnostic. Still, claimed improvements attributed to RCMDT are not disentangled from filtering effects, and the contribution of IES to next-day transfer is not isolated. K–S reporting should include sample sizes and significance levels; including aggregated results across many windows (AM/PM, day-to-day) with error bars would substantiate robustness claims. Efficiency claims for BO vs LHS are expected; a stronger case would compare BO to alternative global optimizers or show that the tail-aware loss measurably improves stress-test outcomes relative to RMSE-only calibration. Comparison with related work (using the summaries provided) Relative to SUMO DT calibration emphasizing distributional metrics on the M50 corridor (Related Work: 2507.10280), this paper’s novelty lies in operator-aware filtering and an assimilation inner loop; however, that M50 work provides broader calibration metrics and systematic distributional analyses that this paper could emulate for completeness. TEVV-centric frameworks (Related Work: 2507.04555) emphasize traceability, continuous validation, and metric suites; RCMDT’s “logbook” and validation-only evidence resonate with TEVV, but the paper would benefit from expanded, standardized validation metrics and formal acceptance criteria. In assimilation, EnKF/IES lines (Related Work: 1209.2736; 1901.06570; 1611.04702; 2312.10503) suggest several alternatives/improvements (IEnKS/EnRML, ILUES, United Filter). A comparative or at least a reasoned choice with hyperparameter details would strengthen technical credibility. Metamodel-based calibration methods (Related Work: 2501.04783) and joint demand–supply calibration frameworks (Related Work: 2501.10934) provide scalable alternatives; while out of scope for buses, the paper could discuss how RCMDT’s outer-loop BO and inner-loop IES compare in sample efficiency and scalability to those metamodel and SPSA-based approaches. Discussion of broader impact and significance If substantiated with broader evidence, the operator-aware audit and decoupled calibration/validation design could improve the robustness and interpretability of operational twins for public transit. The auditing approach raises governance advantages (auditability, reproducibility) but also requires careful threshold governance and transparency to avoid inadvertently discarding valid transport samples in congested or incident conditions. Practical uptake would be enhanced by open-sourcing rule sets, configurations, and providing implementation details for the assimilation loop. Questions for Authors Does the operator audit influence the calibration dataset used by the BO outer loop, or is it applied strictly for validation only? The paper contains statements suggesting both; please clarify the intended protocol and justify it. What are the specific contents/dimensions of the corridor state vector Xcorr and the observation vector y used by the IES? How many ensemble members, how many iterations, what assimilation window, and what were the settings for R and any model-error inflation? How were the audit thresholds (T* = 325 s, v* = 5 km/h) selected? Please provide sensitivity analyses across routes, hours, and days, and report the impact of varying these thresholds on K–S and stress tests. Can you provide an ablation study quantifying the incremental effect of (i) audit on/off, (ii) IES on/off, and (iii) tail-aware composite loss vs RMSE-only, on both calibration fit and validation robustness (including worst-window metrics)? How do you define “Pass/Fail” for K–S? Please report sample sizes, critical values or p-values, and a priori acceptance thresholds, and aggregate results across windows (means, confidence intervals). What parameters are included in θstop, their bounds, and dimension? What priors/initialization were used? How many SUMO runs per BO candidate (replications), and what is the total computational budget? Why is validation conducted on speed CDFs while calibration optimizes D2D travel-time errors? Have you evaluated K–S on travel-time distributions as well, and do conclusions hold? Can you compare IES with a Kalman-style smoother baseline (e.g., ES-MDA, EnRML/IEnKS) or more recent methods (ILUES, United Filter) on your testbeds to support the choice? The “worst 15-min” stress test suggests robustness; could you present route-hour heatmaps summarizing worst-window K–S across the entire study to substantiate next-day transfer claims? Do you have examples where the audit incorrectly flags true congestion (e.g., incidents) as non-transport? How would RCMDT adapt in those cases without operator labels? Overall Assessment The paper addresses an important and underappreciated issue in transit digital twin calibration: the contamination of D2D targets by operational semantics and the ensuing identifiability problems. The operator-aware audit and the two-level Bayesian–assimilation loop are conceptually compelling, and the emphasis on distributional validation and stress tests is aligned with robustness goals. However, the current experimental evidence is limited in scope and depth: key components (IES configuration, audit thresholds, loss weights) are insufficiently specified, ablations and baselines are largely absent, and K–S significance is not established. The decoupling between calibration and validation is well motivated but its practical implementation (whether audit affects calibration) is unclear, and some claimed improvements may primarily reflect data filtering rather than improved model fidelity. Overall, I view the contribution as promising but not yet ready for a top-tier venue; a more thorough empirical study with ablations, clearer protocols, and stronger comparative baselines would significantly strengthen the case.